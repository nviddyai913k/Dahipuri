NLP

1. Write a practical Program to perform tokenization over word and sentence on English and Hindi Text.

!pip install nltk
import nltk
nltk.download('all')

from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer, TreebankWordTokenizer

dataset=""" (Add English texts under quotes) """

print (sent_tokenize(dataset))
for i in sent_tokenize(dataset):
  print(i)

print(word_tokenize(dataset))
for i in word_tokenize(dataset):
  print(i)

print("word_tokenize",word_tokenize(dataset))

tokenizer=TreebankWordTokenizer()
print("TreebankwordTokenizer",tokenizer.tokenize(dataset))

text=""" (Add Hindi texts under quotes) """

print("word_tokenize",word_tokenize(text))

print("sentence_tokenize",sent_tokenize(text))

tokenizer=WordPunctTokenizer()
print("WordPuntTokenizer",tokenizer.tokenize(text))

tokenize=TreebankWordTokenizer()
print("TreebankWordTokenizer",tokenize.tokenize(text))


2. Write a Program to identify Stopwords in a given sentence in English.

!pip install nltk
import nltk
nltk.download('all')

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

dataset=""" (Add English texts under quotes) """

stop_words=set(stopwords.words('english'))
print(stop_words)

print("Total count of Stopwords:",len(dataset))

words=word_tokenize(dataset)
print(words)

print("Total words:",len(words))

filtered_sentence=[]
for w in words:
  if w not in stop_words:
    filtered_sentence.append(w)
print(filtered_sentence)
print()
print("After removing stopwords",len(filtered_sentence))


3. Write a program to perform Stemming and Lemmatization for English Text.

!pip install nltk
import nltk
nltk.download('all')

from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize

ps = PorterStemmer()

words = ["program", "programs", "programmer", "programming", "programmers"]

for w in words:
  print(w, " : ", ps.stem(w))

sentence="Programmers program with programming languages"
words = word_tokenize(sentence)

for w in words:
  print(w, " : ", ps.stem(w))


e_words = ["wait", "waiting", "waited", "waits"]
ps = PorterStemmer()
for w in e_words:
  rootWord=ps.stem(w)
  print(rootWord)

sentence="(Add English texts under quotes)"
words = word_tokenize(sentence)
ps = PorterStemmer()
for w in words:
  rootWord=ps.stem(w)
  print(w,rootWord)

porter = PorterStemmer()
lancaster = LancasterStemmer()

print("Porter Stemmer")
print(porter.stem("cats"))
print(porter.stem("trouble"))
print(porter.stem("troubling"))
print(porter.stem("troubled"))
print()

print("Lancaster Stemmer")
print(lancaster.stem("cats"))
print(lancaster.stem("trouble"))
print(lancaster.stem("troubling"))
print(lancaster.stem("troubled"))


word_list = ["friend", "friendship", "friends", "friendships","stabil","destabilize","misunderstaning","railroad","moonlight","football"]
print("{0:20}{1:20}{2:20}".format("Word","Porter Stemmer","lancaster Stemmer"))
for word in word_list:
  print("{0:20}{1:20}{2:20}".format(word,porter.stem(word),lancaster.stem(word)))

sentence="Pythoners are very intelligent and work very pythonly and now they are pythoning success"
porter.stem(sentence)

porter_stemmer = PorterStemmer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print("Stemming for {} is {}".format(w,porter_stemmer.stem(w)))

wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print("Lemma for {} is {}".format(w, wordnet_lemmatizer.lemmatize(w)))


4. Write a program to segregate Part Of Speech (POS Tagging) for English Text.

!pip install nltk
import nltk
nltk.download('all')

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

dataset=""" (Add English texts under quotes) """

new_data =word_tokenize(dataset)
new_data

pos_tag(new_data)

nltk.help.upenn_tagset()


5. Write a program to perform Name Entity Recognition (NER) & Chunking on English Text.

!pip install nltk
import nltk
nltk.download('all')

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk, RegexpParser

dataset=""" (Add English texts under quotes) """

dataset_tag= pos_tag(word_tokenize(dataset))
dataset_tag

#Apply NER with ne_chunk
data_ner=ne_chunk(dataset_tag)
print(data_ner)

(Chunking)

dataset=""" (Add English texts under quotes) """

new_data=word_tokenize(dataset)
print(new_data)

postagging=pos_tag(new_data)
print(postagging)

sequence_chunk=""" chunk: {<NNPS>+}
{<NNP>+}
{<NN>+}"""

chunk=RegexpParser(sequence_chunk)

chunk_result=chunk.parse(postagging)
print(chunk_result)


6. Write a program to perform WordNet & also check Word Similarity on English Text.

!pip install nltk
import nltk
nltk.download('all')

from nltk.corpus import wordnet, wordnet as wn

syns=wordnet.synsets("program")
print(syns)
print(syns[0])

print(syns[0].lemmas())
print(syns[0].lemmas()[0].name())

print(syns[0].definition())

print(syns[0].examples())

antonyms = []
def TofindAntonyms(x):
  for syn in wordnet.synsets(x):
    for lm in syn.lemmas():
      if lm.antonyms():
        antonyms.append(lm.antonyms()[0].name()) #Adding into antonyms
        return antonyms

print(set(TofindAntonyms("bright")))

print(set(TofindAntonyms("inactive")))

print(set(TofindAntonyms("good")))

synonyms=[]
antonyms=[]
for syn in wordnet.synsets("good"):
  for l in syn.lemmas():
    synonyms.append(l.name())
    if l.antonyms():
      antonyms.append(l.antonyms()[0].name())
      print("Synonyms:",(set(synonyms)))
      print("Antonyms:",(set(antonyms)))

(WordSimilarity)

car=wordnet.synset('car.n.01')
automobile=wordnet.synset('automobile.n.01')

print("Similarity between car and automobile",car.path_similarity(automobile))

(Wu-Palmer Similarity)

w1=wordnet.synset('run.v.01')# v here denotes the tag verb
w2= wordnet.synset('sprint.v.01')
print(w1.wup_similarity(w2))

jump=wn.synset('jump.v.01')
leap=wn.synset('leap.v.01')
ship =wn.synset('ship.n.01')

print ("similarity between jump and leap ",jump.wup_similarity(leap))

7. Write a program to implement word cloud of English Text.

!pip install nltk
import nltk
nltk.download('all')

from wordcloud import WordCloud

import matplotlib.pyplot as plt

import matplotlib.pyplot as pPlot
from wordcloud import WordCloud, STOPWORDS
import numpy as npy
from PIL import Image
dataset = open("/content/movie_list.txt","r").read()
dataset = dataset.upper()

maskArray = npy.array(Image.open("/content/video-300x300.jpg"))
cloud = WordCloud(background_color = "#000000", min_font_size=5, colormap='PuRd_r',max_font_size=70,font_path='/content/LEMONMILK-Medium.otf',max_words = 2000, width = 1400, height=1400,collocations=True,mask = maskArray, stopwords= set(STOPWORDS)).generate(dataset)

plt.figure(figsize = (10,10), facecolor = None)
plt.imshow(cloud)
plt.axis("off")

8. Write a program to process Text Summarization.

!pip install nltk
import nltk
nltk.download('all')

from nltk.tokenize import word_tokenize, sent_tokenize

dataset=open("(Paste the copied pth under parenthesis)", encoding='cp1252').read()

dataset

def summarization(dataset):
  result = []
  for number, sentence in enumerate(sent_tokenize(dataset)):
    number_tokens = len(word_tokenize(sentence))
    tagged = nltk.pos_tag(word_tokenize(sentence))
    number_nouns= len([word for word, pos in tagged if pos in ["NN", "NNP"]])
    ners=nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=False)
    number_ners=len([chunk for chunk in ners if hasattr(chunk,'label')])
    score=(number_ners + number_nouns) / float(number_tokens)
    result.append((number, score, sentence))
    return result

summ = summarization(dataset)
summ

for i in sorted(summ, key=lambda x: x[1], reverse=True):
  print(i[2])


9. Write a program to implement Word2Vec on Wikipedia Articles and finding the similarity between the words.

!pip install wikipedia
!pip install gensim
!pip install nltk
import nltk
nltk.download('all')

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import string
import warnings
from gensim.models import Word2Vec
warnings.filterwarnings(action="ignore", category = UserWarning, module="gensim")

import wikipedia
from wikipedia import search
from wikipedia import page

titles = search("Data Science")
wikipage = page(titles[0])
wikipage.content
wikipage.categories
wikipedia.summary("Data Science", sentences = 1)
wikipedia.search("Data Science")

def preprocessing(text):
  result = []
  sent = sent_tokenize(text)
  for sentence in sent:
    words = word_tokenize(sentence)
    tokens = [w for w in words if w.lower() not in string.punctuation]
    stopw = stopwords.words("english")
    tokens = [token for token in tokens if token not in stopw]
    tokens = [word for word in tokens if len(word)>= 3]
    lemma = WordNetLemmatizer()
    tokens = [lemma.lemmatize(word) for word in tokens]
    result += [tokens]
    return result

text_p = preprocessing(wikipage.content)
text_p[0]

min_count = 2
size = 50
window = 4

10. Write a program to Train a model for Movie Review Classification using NLP Techniques.

!pip install nltk
import nltk
nltk.download('all')

import random
from nltk.corpus import movie_reviews

documents = []

for category in movie_reviews.categories():
  for fileid in movie_reviews.fileids(category):
    documents.append((list(movie_reviews.words(fileid)), category))

random.shuffle(documents)

print(documents[1])

all_words = []
for w in movie_reviews.words():
  all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)
print(all_words.most_common(15))
print(all_words["love"])

word_features = list(all_words.keys())[:3000]

def find_features(document):
  words = set(document)
  features = {}
  for w in word_features:
    features[w] = (w in words)
    return features

print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))
featuresets = [(find_features(rev), category)
for (rev, category) in documents]

training_set = featuresets[ :1900]
testing_set = featuresets[1900: ]

classifier = nltk.NaiveBayesClassifier.train(training_set)

print("Accuracy:",
(nltk.classify.accuracy(classifier, testing_set))*100)

classifier.show_most_informative_features(15)
