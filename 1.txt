Practical No. 01(A) - Methods to Handle Missing Values (Placement_Dataset.csv) - (14 Cells of Code)

(Imputation)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data_set = pd.read_csv('Placement_Dataset.csv')

data_set.head(10)

data_set.shape

data_set.isnull().sum()

fig, ax = plt.subplots(figsize=(8,8))
sns.distplot(data_set.salary)

data_set['salary'].fillna(data_set['salary'].median(),inplace=True)

data_set.isnull().sum()

(Dropping Method)

salary_dataset = pd.read_csv('Placement_Dataset.csv')

salary_dataset.shape

salary_dataset.isnull().sum()

salary_dataset = salary_dataset.dropna(how='any')

salary_dataset.isnull().sum()

salary_dataset.shape






Practical No. 01(B) â€“ Label Encoding (data.csv, iris_data.csv) - (15 Cells of Code)

(data.csv)

import pandas as pd 
from sklearn.preprocessing import LabelEncoder

cancer_data=pd.read_csv('data.csv')

cancer_data.head(10)

cancer_data['diagnosis'].value_counts()

label_encode=LabelEncoder()

labels=label_encode.fit_transform(cancer_data.diagnosis)

cancer_data['target']=labels

cancer_data.head()

cancer_data['target'].value_counts()

(iris_data.csv)

iris_data=pd.read_csv('iris_data.csv')

iris_data.head()

iris_data['Species'].value_counts()

label_encoder_1 = LabelEncoder()
iris_labels = label_encoder_1.fit_transform(iris_data.Species)
iris_data['target']= iris_labels

iris_data.head()

iris_data['target'].value_counts()





Practical No. 02 - Linear Regression (housingData-Real.csv) - (25 Cells of Code)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sbn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score,mean_squared_error

data = pd.read_csv('housingData-Real.csv')

data.info()

data.head()

data.describe()

data.isnull().sum()

data['date']=pd.to_datetime(data['date'])

data.info()

data.head()

data.drop(['id','date'],axis=1,inplace=True)

data.head()

(Univariate Linear Regression)

X=data.sqft_living
Y=data.price

X=np.array(X).reshape(-1,1)
Y=np.array(Y).reshape(-1,1)
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=101)

model1=LinearRegression()
model1.fit(X_train,Y_train)

Y_pred = model1.predict(X_test)

a=r2_score(Y_test,Y_pred)

a

plt.scatter(X_train,Y_train)
plt.plot(X_train, model1.predict(X_train),color='red')

plt.scatter(X_test,Y_test)
plt.plot(X_train, model1.predict(X_train),color='red')

(Multiple Linear Regression)

x=data.drop(['price'],axis=1)
y=data.price

x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1)

models=LinearRegression()
model =models.fit(x_train,y_train)

y_predict=models.predict(x_test)

b=r2_score(y_test,y_predict)
b

print("r2 score of the Univariate linear Regression is: {}".format(a))
print("r2 score of the Multiple linear Regression is: {}".format(b))





Practical No. 03 - Logistic Regression (mail_data.csv) - (20 Cells of Code)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

raw_mail_data=pd.read_csv('mail_data.csv')

print(raw_mail_data)

mail_data=raw_mail_data.where((pd.notnull(raw_mail_data)),'')

mail_data.head()

mail_data.shape

mail_data.loc[mail_data['Category']=='spam','Category',] = 0
mail_data.loc[mail_data['Category']=='ham','Category',] = 1

X=mail_data['Message']
Y=mail_data['Category']

print(X)

print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=3)

print(X.shape)
print(X_train.shape)
print(X_test.shape)

feature_extraction = TfidfVectorizer(min_df=1,stop_words='english')
X_train_features=feature_extraction.fit_transform(X_train)
X_test_features=feature_extraction.transform(X_test)
Y_train=Y_train.astype('int')
Y_test=Y_test.astype('int')

model = LogisticRegression()

model.fit(X_train_features, Y_train)

prediction_on_training_data=model.predict(X_train_features)
accuracy_on_training_data=accuracy_score(Y_train, prediction_on_training_data)

print('Accuracy on trainging data: ',accuracy_on_training_data)

prediction_on_test_data=model.predict(X_test_features)
accuracy_on_test_data=accuracy_score(Y_test, prediction_on_test_data)

print('Accuracy on test data :', accuracy_on_test_data)

input_mail=["The House of Dior was established on 16 December 1946 at 30 Avenue Montaigne in Paris. However, the current Dior corporation celebrates 1947 as the opening year. Dior was financially backed by wealthy businessman Marcel Boussac."]
input_data_features = feature_extraction.transform(input_mail)
prediction = model.predict(input_data_features)
print(prediction)
if(prediction[0]==1):
    print('Ham mail')    
else:
    print('Spam mail')





Practical No. 04 - Understanding and Learning Feature Transformation (titanic.csv) - (44 Cells of Code)

import pandas as pd
df=pd.read_csv('titanic.csv', usecols=['Pclass','Age','Fare','Survived'])
df.head()

df['Age'].fillna(df.Age.median(),inplace=True)

df.isnull().sum()

X=df.iloc[:,1:]
y=df.iloc[:,0]

X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

X_train

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
X_train_scaled=scaler.fit_transform(X_train)

X_train_scaled

X_test_scaled=scaler.transform(X_test)

X_test_scaled

from sklearn.linear_model import LogisticRegression
classification=LogisticRegression()

classification.fit(X_train_scaled,y_train)

classification.predict(X_test_scaled)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.preprocessing import MinMaxScaler
min_max=MinMaxScaler()
df_minmax=pd.DataFrame(min_max.fit_transform(X_train))
df_minmax.head()

import matplotlib.pyplot as plt
import seaborn as sns
sns.pairplot(df_minmax)

plt.hist(df_minmax[1],bins=20)

plt.hist(df_minmax[2],bins=20)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.preprocessing import RobustScaler
scaler=RobustScaler()
df_robust_scaler=pd.DataFrame(scaler.fit_transform(X_train))
df_robust_scaler.head()

scaler.transform(X_test)

import seaborn as sns
sns.pairplot(df_robust_scaler)

import seaborn as sns
sns.pairplot(df)

plt.hist(df_robust_scaler[1],bins=20)

plt.hist(df_robust_scaler[2],bins=20)

df=pd.read_csv('titanic.csv',usecols=['Age','Fare','Survived'])
df.head()

df['Age']=df['Age'].fillna(df['Age'].median())

df.isnull().sum()

import matplotlib.pyplot as plt

import scipy.stats as stat
import pylab

def plot_data(df,feature):
    plt.figure(figsize=(10,6))
    plt.subplot(1,2,1)
    df[feature].hist()
    plt.subplot(1,2,2)
    stat.probplot(df[feature],dist='norm',plot=pylab)
    plt.show()

plot_data(df,'Age')

import numpy as np
df['Age_log']=np.log(df['Age'])
plot_data(df,'Age_log')

df['Age_reciprocal']=1/df.Age
plot_data(df,'Age_reciprocal')

df['Age_sqaure']=df.Age**(1/2)
plot_data(df,'Age_sqaure')

df['Age_exponential']=df.Age**(1/1.2)
plot_data(df,'Age_exponential')

df['Age_Boxcox'],parameters=stat.boxcox(df['Age'])

print(parameters)

plot_data(df,'Age_Boxcox')

plot_data(df,'Fare')

df['Fare_log']=np.log1p(df['Fare'])
plot_data(df,'Fare_log')

df['Fare_Boxcox'],parameters=stat.boxcox(df['Fare']+1)
plot_data(df,'Fare_Boxcox')





Practical No. 05 - Decision Tree - (19 Cells of Code)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.tree import plot_tree

df=sns.load_dataset('iris')
df.head

df.info()

df.shape

df.isnull().any()

sns.pairplot(data=df, hue='species')

sns.heatmap(df.corr())

target=df['species']
df1=df.copy()
df1=df1.drop('species', axis = 1)

X=df1

X

target

le = LabelEncoder()
target = le.fit_transform(target)
target

y= target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)
print("Training split input- ", X_train.shape)
print()
print("Testing split input- ", X_test.shape)

dtree=DecisionTreeClassifier()
dtree.fit(X_train,y_train)
print('Decision Tree Classifier Created')

y_pred = dtree.predict(X_test)
print("Classification report - \n", classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,5))
sns.heatmap(data=cm, linewidths=.5, annot=True, square=True, cmap='Blues')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title='Accuracy Score:{0}'.format(dtree.score(X_test, y_test))
plt.title(all_sample_title, size=15)

plt.figure(figsize=(20,20))
dec_tree = plot_tree(decision_tree = dtree, feature_names = df1.columns, class_names = ["setosa", "vercicolor", "verginica"] , filled = True, precision = 4, rounded = True)





Practical No. 06 - K-Means Clustering (Mall_Customers.csv) - (18 Cells of Code)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('Mall_Customers.csv')

dataset.head()

dataset.tail()

dataset.shape

dataset.info()

dataset.describe()

dataset.isnull().sum()

dataset.columns

data=dataset[['Annual_Income_(k$)','Spending_Score']]

data

data.shape

data.values

X = dataset.iloc[:, [2, 3]].values

from sklearn.cluster import KMeans
wc_ss = []
for i in range(1, 11):
    kmeans_clu = KMeans(n_clusters = i, random_state = 56)
    kmeans_clu.fit(X)
    wc_ss.append(kmeans_clu.inertia_)

plt.figure(figsize=(10,5))
plt.plot(range(1,11), wc_ss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters = 5, random_state = 56)
y_kmeans = kmeans.fit_predict(X)

plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'pink', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'yellow', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'red', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'orange', label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'green', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'violet', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()





Practical No. 07 - Random Forest Regressor (gld_price_data.csv) - (21 Cells of Code)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics

gold_data = pd.read_csv('gld_price_data.csv')

gold_data.head()

gold_data.info()

gold_data.isnull().sum()

gold_data.describe()

correlation = gold_data.corr()

plt.figure(figsize=(8,8))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Reds')

print(correlation['GLD'])

sns.distplot(gold_data['GLD'], color='red')

X = gold_data.drop(['Date','GLD'], axis=1)
Y = gold_data['GLD']

print(X)

print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)

regressor = RandomForestRegressor(n_estimators=100)

regressor.fit(X_train, Y_train)

test_data_prediction = regressor.predict(X_test)

print(test_data_prediction)

error_score = metrics.r2_score(Y_test, test_data_prediction)
print("R square error:", error_score)

Y_test = list(Y_test)

plt.plot(Y_test, color='red', label='Actual Value')
plt.plot(test_data_prediction, color='black', label='Predicted Value')
plt.title('Actual Price vs Predicted Price')
plt.xlabel('Number of values')
plt.ylabel('GLD PRice')
plt.legend()
plt.show()





Practical No. 08 - Time Series Using - Darts Library - (12 Cells of Code)

!pip install darts

!pip install pyyaml==5.4.1

from darts.datasets import AirPassengersDataset, MonthlyMilkDataset

AirPassengersDataset().load().pd_series()

import matplotlib.pyplot as plt
series_air = AirPassengersDataset().load()
series_milk = MonthlyMilkDataset().load()
series_air.plot(label='Number of air passengers')
series_milk.plot(label='Pounds of milk produced per cow')
plt.legend();

from darts.dataprocessing.transformers import Scaler
scaler_air, scaler_milk = Scaler(), Scaler()
series_air_scaled = scaler_air.fit_transform(series_air)
series_milk_scaled = scaler_milk.fit_transform(series_milk)
series_air_scaled.plot(label='air')
series_milk_scaled.plot(label='milk')
plt.legend();

train_air, val_air = series_air_scaled[:-36], series_air_scaled[-36:]
train_milk, val_milk = series_milk_scaled[:-36], series_milk_scaled[-36:]

from darts import TimeSeries
from darts.utils.timeseries_generation import gaussian_timeseries, linear_timeseries, sine_timeseries
from darts.models import RNNModel, TCNModel, TransformerModel, NBEATSModel, BlockRNNModel
from darts.metrics import mape, smape

model_air_milk = NBEATSModel(input_chunk_length=24, output_chunk_length=12, n_epochs=100, random_state=0)

model_air_milk.fit([train_air, train_milk], verbose=True)

pred = model_air_milk.predict(n=36, series=train_air)
series_air_scaled.plot(label='actual')
pred.plot(label='forecast')
plt.legend();
print('MAPE = {:.2f}%'.format(mape(series_air_scaled, pred)))

pred = model_air_milk.predict(n=36, series=train_milk)
series_milk_scaled.plot(label='actual')
pred.plot(label='forecast')
plt.legend();
print('MAPE = {:.2f}%'.format(mape(series_milk_scaled, pred)))